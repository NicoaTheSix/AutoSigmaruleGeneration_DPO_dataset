{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba3e7ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re,os,time,json,random,subprocess,ollama,argparse,pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import uuid\n",
    "import argparse\n",
    "import sys\n",
    "import secrets\n",
    "import string\n",
    "\n",
    "def generate_secure_random_string(length):\n",
    "    characters = string.ascii_letters + string.digits\n",
    "    return ''.join(secrets.choice(characters) for _ in range(length))\n",
    "# Encoding\n",
    "file_encoding = \"utf-8\"\n",
    "# Paths\n",
    "\"\"\"\n",
    "when using Github Desktop to open the source directory\n",
    "home path should be \\'...\\\\Malicious_Code_Generation \\'\n",
    "home_path is the main directory of the project\n",
    "\n",
    "The name of responce_title is based on the time.\n",
    "\"\"\" \n",
    "source                  =\"openai\"#\"ollama\"\n",
    "llm                     =\"gpt-4o-mini\"#\"llama3.2\"\n",
    "home_path               = os.getcwd()\n",
    "response_title          = time.strftime(\"%Y%m%d%H%M\",time.localtime())\n",
    "response_folder_path    = os.path.join(home_path,\"response\")\n",
    "targetOS               =\"Windows 11\"\n",
    "def Llmrequest(messages:list=[{\"role\":\"user\",\"content\":\"nothing to say\"}],source:str=source,llm:str=llm):\n",
    "    \"\"\" \n",
    "    目前有Openai api與Ollama 套件作為LLM來源\n",
    "    \"\"\"\n",
    "        \n",
    "    def gpt_request(llm:str=\"gpt-4o-mini\",messages: list=[{\"role\":\"user\",\"content\":\"nothing to say\"}]):\n",
    "        # Get Response from GPT\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        client = OpenAI(api_key = api_key)\n",
    "        response = client.chat.completions.create(\n",
    "        model = llm,\n",
    "        messages = messages,\n",
    "        temperature = 0\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    def ollama_request(llm:str = \"llama3.2\", messages:list = [{\"role\":\"user\",\"content\":\"nothing to say\"}]):\n",
    "        response= ollama.chat(\n",
    "        model=llm,\n",
    "        messages=messages\n",
    "        )\n",
    "        return response['message']['content']\n",
    "    if source == \"openai\":\n",
    "        return gpt_request(llm=llm,messages=messages)\n",
    "    elif source == \"ollama\" :\n",
    "        return ollama_request(llm=llm,messages=messages)\n",
    "def Llminteraction(dict_input:dict={},task:str=\"\",outputFormatRegex:str=\"\",display:bool=False,suffix:str=\"\"):\n",
    "    print(f\"[+]Start {task}.\")\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": textLoader(txtName=f\"SystemPrompt_{task}\")},\n",
    "        {\"role\": \"user\", \"content\": \"<text>\"+str(dict_input)+\"</text>\"}]\n",
    "    if display:print(messages)\n",
    "    output=Llmrequest(messages)\n",
    "    print(f\"[-]Finish {task}.\")\n",
    "    if outputFormatRegex != \"\":\n",
    "        outputPattern=re.compile(r'<output>(.*?)</output>', re.DOTALL) \n",
    "        outputContent = outputPattern.findall(output)\n",
    "        for i in range(len(outputContent)):\n",
    "            with open(os.path.join(os.getcwd(),\"response\",suffix+\"_\"+task+\"_\"+str(i)+\".yaml\"),\"w\",encoding='utf-8')as f:\n",
    "                f.write(outputContent[i])\n",
    "        return outputContent\n",
    "    \n",
    "    with open(os.path.join(os.getcwd(),\"response\",suffix+\"_\"+task+\".yaml\"),\"w\",encoding='utf-8')as f:\n",
    "        f.write(output)\n",
    "    return output\n",
    "\"\"\"\n",
    "fuction : Load the prompt from thedirectory :Prompt:\n",
    "\"\"\"\n",
    "def textLoader(txtName:str):\n",
    "    with open(os.path.join(os.getcwd(),\"prompt\",txtName+\".txt\"),\"r\",encoding=\"utf-8\")as file:\n",
    "        prompt=file.read()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c453802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1005\n",
      "[+]Start attackPhaseAnalysis_COSTAR.\n",
      "[-]Finish attackPhaseAnalysis_COSTAR.\n",
      "[+]Start attackPatternInferencer_COSTAR.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "with open(os.path.join(os.getcwd(),\"data\",\"mitre_ttp_data.json\"),\"r\",encoding='utf-8')as f:\n",
    "    #print(type(f.read()))\n",
    "    ttp=json.load(f)\n",
    "techniques = [\n",
    "    \"T1005\",\"T1007\",\"T1016\",\"T1021.001\",\"T1033\",\"T1036.004\",\"T1046\",\"T1047\",\"T1049\",\"T1053.005\",\"T1055.002\",\"T1059.001\",\"T1069.001\",\"T1071.001\",\"T1082\",\"T1083\",\"T1087.001\",\"T1105\",\"T1112\",\"T1204.002\",\"T1219\",\"T1491\",\"T1518.001\",\"T1547.001\",\"T1547.009\",\"T1548.002\",\"T1562.001\",\"T1564.003\",\"T1566.001\",\"T1567\"]\n",
    "\n",
    "for i in techniques:\n",
    "    print(i)\n",
    "    TTP_ID=i\n",
    "    TTP_Description=ttp[i][\"Description\"]\n",
    "    pe=ttp[i][\"Procedure Examples\"]\n",
    "    filename=generate_secure_random_string(10)\n",
    "    attackPhase=Llminteraction(dict_input={\"TTP_ID\": {TTP_ID},\"TTP_Description\": {TTP_Description},\"procedure example\":pe},task=\"attackPhaseAnalysis_COSTAR\",suffix=i+\"_\"+filename)\n",
    "    attackPattern=Llminteraction(dict_input={\"attackPhase\":attackPhase},task=\"attackPatternInferencer_COSTAR\",outputFormatRegex=r'<output>(.*?)</output>',suffix=i+\"_\"+filename+\"_COSTAR\")\n",
    "    for j in range(len(attackPattern)):\n",
    "        logInference=Llminteraction(dict_input={\"attackPattern\":attackPattern[j]},task=\"logInferencer_COSTAR\",suffix=i+\"_\"+filename+\"_\"+str(j))\n",
    "        sigmarule=Llminteraction(dict_input={\"logInference\":logInference},task=\"sigmaRuleGenerator_COSTAR\",suffix=i+\"_\"+filename+\"_\"+str(j))\n",
    "        KQL=Llminteraction(dict_input={\"sigmarule\":sigmarule},task=\"KQLGeneration\",outputFormatRegex=r'<output>(.*?)</output>',suffix=i+\"_\"+filename+\"_COSTAR_\"+str(j))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e305d1",
   "metadata": {},
   "source": [
    "### 製作偏好資料集\n",
    "製作方法：利用大型語言模型「生成有特定缺陷的輸出」作為DPO所需偏好資料集中的「偏惡資料」"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a0ec1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1005\n",
      "[+]Start attackPhaseAnalysis_COSTAR.\n"
     ]
    }
   ],
   "source": [
    "#生成可能的缺陷面向\n",
    "#依據不同的缺陷末向\n",
    "import os\n",
    "import json\n",
    "with open(os.path.join(os.getcwd(),\"data\",\"mitre_ttp_data.json\"),\"r\",encoding='utf-8')as f:\n",
    "    #print(type(f.read()))\n",
    "    ttp=json.load(f)\n",
    "techniques = [\"T1005\"]\n",
    "\n",
    "for i in techniques:\n",
    "    print(i)\n",
    "    TTP_ID=i\n",
    "    TTP_Description=ttp[i][\"Description\"]\n",
    "    pe=ttp[i][\"Procedure Examples\"]\n",
    "    \n",
    "    filename=generate_secure_random_string(10)\n",
    "    #針對第一階段的提示詞生成可能的缺陷面向\n",
    "    #生成一次正常的輸出\n",
    "    attackPhase=Llminteraction(\n",
    "        dict_input={\n",
    "            \"TTP_ID\": {TTP_ID},\n",
    "            \"TTP_Description\": {TTP_Description},\n",
    "            \"procedure example\":pe},\n",
    "        task=\"attackPhaseAnalysis_COSTAR\",\n",
    "        suffix=i+\"_\"+filename)\n",
    "    DefectAspectGeneration_attackPhase=Llminteraction(\n",
    "        dict_input={\n",
    "            \"prompt\":textLoader(txtName=f\"SystemPrompt_attackPhaseAnalysis_COSTAR\"),\n",
    "            \"TTP_ID\": {TTP_ID},\n",
    "            \"TTP_Description\": {TTP_Description},\n",
    "            \"procedure example\":pe},\n",
    "        task=\"DefectAspectGeneration\",\n",
    "        suffix=i+\"_\"+filename)\n",
    "    #\n",
    "\n",
    "    \n",
    "    DefectAspectGeneration_attackPattern=Llminteraction(\n",
    "        dict_input={\n",
    "            \"prompt\":textLoader(txtName=f\"SystemPrompt_attackPatternInferencer_COSTAR\"),\n",
    "            \"attackPhase\":attackPhase},\n",
    "        task=\"DefectAspectGeneration\",\n",
    "        #outputFormatRegex=r'<output>(.*?)</output>',\n",
    "        suffix=i+\"_\"+filename+\"_COSTAR\")\n",
    "    attackPattern=Llminteraction(\n",
    "        dict_input={\n",
    "            \"attackPhase\":attackPhase},\n",
    "        task=\"attackPatternInferencer_COSTAR\",\n",
    "        #outputFormatRegex=r'<output>(.*?)</output>',\n",
    "        suffix=i+\"_\"+filename+\"_COSTAR\")\n",
    "    j=0\n",
    "    DefectAspectGeneration_logInference=Llminteraction(\n",
    "        dict_input={\n",
    "            \"prompt\":textLoader(txtName=f\"SystemPrompt_logInferencer_COSTAR\"),\n",
    "            \"attackPattern\":attackPattern[j]\n",
    "            },\n",
    "        task=\"DefectAspectGeneration\",\n",
    "        suffix=i+\"_\"+filename+\"_\"+str(j))\n",
    "    logInference=Llminteraction(\n",
    "        dict_input={\"attackPattern\":attackPattern[j]},\n",
    "        task=\"logInferencer_COSTAR\"\n",
    "        ,suffix=i+\"_\"+filename+\"_\"+str(j))\n",
    "        \n",
    "    DefectAspectGeneration_sigmarule_DefectAspectGeneration=Llminteraction(\n",
    "        dict_input={\n",
    "            \"prompt\":textLoader(txtName=f\"SystemPrompt_sigmaRuleGenerator_COSTAR\"),\n",
    "            \"logInference\":logInference},\n",
    "        task=\"DefectAspectGeneration\",\n",
    "        suffix=i+\"_\"+filename+\"_\"+str(j))\n",
    "    sigmarule=Llminteraction(\n",
    "        dict_input={\"logInference\":logInference},\n",
    "        task=\"sigmaRuleGenerator_COSTAR\",\n",
    "        suffix=i+\"_\"+filename+\"_\"+str(j))\n",
    "        \n",
    "    DefectAspectGeneration_KQL=Llminteraction(\n",
    "        dict_input={\"sigmarule\":sigmarule},\n",
    "        task=\"KQLGeneration\",\n",
    "        outputFormatRegex=r'<output>(.*?)</output>',\n",
    "        suffix=i+\"_\"+filename+\"_COSTAR_\"+str(j))\n",
    "    KQL=Llminteraction(\n",
    "        dict_input={\"sigmarule\":sigmarule},\n",
    "        task=\"KQLGeneration\",\n",
    "        outputFormatRegex=r'<output>(.*?)</output>',\n",
    "        suffix=i+\"_\"+filename+\"_COSTAR_\"+str(j))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autosigmagen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
